{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Overview \n",
    "\n",
    "1. TF Serving Overview & Architecture\n",
    "\n",
    "2. Review & launch model server\n",
    "\n",
    "3. Predict using Tensorflow Serving\n",
    "\n",
    "Before you continue:\n",
    "\n",
    "+ Please make sure you have installed docker in your enviornment.\n",
    "+ Ensure your trained model exists (exported for TF Serving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Serving\n",
    "\n",
    "[TensorFlow Serving](https://www.tensorflow.org/tfx/serving/) is an open-source software library for serving machine learning models. Here are some features of TF Serving:\n",
    "\n",
    "1. high performance model hosting system. Designed for primarily for synchronous inference but also supports bulk-processing (e.g. map-reduce) in production enviornments. \n",
    "\n",
    "2. TF Serving includes support for model lifecycle management. Multiple models, or multiple versions of the same model can be served simultaneously.\n",
    " \n",
    "3. Facilitates canarying new versions, migrating clients to new models or versions, and A/B testing experimental models.\n",
    "\n",
    "4. TensorFlow Serving comes with a scheduler that groups individual inference requests into batches for joint execution on a GPU, with configurable latency controls.\n",
    "\n",
    "5. TensorFlow Serving has out-of-the-box support for TensorFlow models. \n",
    "\n",
    "6. In addition to trained TensorFlow models, TF servables can include other assets needed for inference such as embeddings, vocabularies and feature transformation configs, or even non-TensorFlow-based machine learning models.\n",
    "\n",
    "7. The architecture is highly modular. You can use some parts individually (e.g. batch scheduling) or use all the parts together. \n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "[Servables](https://www.tensorflow.org/tfx/serving/overview#servables) are the underlying objects that clients use to perform computation (for example, a lookup or inference). Servables do not manage their own lifecycle & include the following:\n",
    "\n",
    "+ a TensorFlow SavedModelBundle (tensorflow::Session)\n",
    "+ a lookup table for embedding or vocabulary lookups\n",
    "\n",
    "\n",
    "[Servable Versions](https://www.tensorflow.org/tfx/serving/overview#servable_versions) TensorFlow Serving can handle one or more versions of a servable over the lifetime of a single server instance. \n",
    "\n",
    "[Servable Streams](https://www.tensorflow.org/tfx/serving/overview#servable_streams) A servable stream is the sequence of versions of a servable, sorted by increasing version numbers.\n",
    "\n",
    "[Models](https://www.tensorflow.org/tfx/serving/overview#models) TensorFlow Serving represents a model as one or more servables.\n",
    "\n",
    "[Loaders](https://www.tensorflow.org/tfx/serving/overview#loaders) manage a servable's life cycle. \n",
    "\n",
    "[Sources](https://www.tensorflow.org/tfx/serving/overview#sources) are plugin modules that find and provide servables.\n",
    "\n",
    "[Aspired versions](https://www.tensorflow.org/tfx/serving/overview#aspired_versions) represent the set of servable versions that should be loaded and ready. \n",
    "\n",
    "[Managers](https://www.tensorflow.org/tfx/serving/overview#managers) handle the full lifecycle of Servables, including:\n",
    "\n",
    "+ loading Servables\n",
    "+ serving Servables\n",
    "+ unloading Servables\n",
    "\n",
    "Managers listen to Sources and track all versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../assets/tf_serving_servable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Server\n",
    "\n",
    "We will want to serve our Tensorflow model using docker. Please ensure you have installed Docker installed. For more details, visit [Using TensorFlow Serving with Docker](https://www.tensorflow.org/tfx/serving/docker). You will also need to install `pip install grpcio grpcio-tools` or install the package dependencies included in `requirements.txt` available in this repo.\n",
    "\n",
    "### Running a serving image\n",
    "The serving images (both CPU and GPU) have the following properties:\n",
    "\n",
    "```\n",
    "Port 8500 exposed for gRPC\n",
    "Port 8501 exposed for the REST API\n",
    "Optional environment variable MODEL_NAME (defaults to model)\n",
    "Optional environment variable MODEL_BASE_PATH (defaults to /models)\n",
    "```\n",
    "\n",
    "\n",
    "If you look through the source code for `start_model_server.sh`, you'll observe the following CLI command which runs the Docker container, publish the container's ports to your host's ports, and mounting your host's path to the SavedModel to where the container expects models...\n",
    "\n",
    "```\n",
    "docker run -it \\\n",
    "  -p 127.0.0.1:$HOST_PORT:$CONTAINER_PORT \\\n",
    "  -v $MODEL_BASE_PATH:$CONTAINER_MODEL_BASE_PATH \\\n",
    "  -e MODEL_NAME=nyc-taxi\\\n",
    "  --rm $DOCKER_IMAGE_NAME\n",
    "```\n",
    "\n",
    "#### The following command is an example for how to run the model server...\n",
    "\n",
    "```bash ./start_model_server.sh```\n",
    "\n",
    "#### Output...\n",
    "\n",
    "```\n",
    "Download TF Serving docker image: tensorflow/serving\n",
    "Using default tag: latest\n",
    "latest: Pulling from tensorflow/serving\n",
    "Digest: sha256:1aaf111b4abb9f2aee618d13f556ab24fee4fff4c44993683772643a7c513b1d\n",
    "Status: Image is up to date for tensorflow/serving:latest\n",
    "Starting the Model Server to serve from: /Users/arm/code/tfx/oreilly/tf/run_0/serving_model_dir/export/nyc-taxi\n",
    "Model directory: /Users/arm/code/tfx/oreilly/tf/run_0/serving_model_dir/export/nyc-taxi\n",
    "2019-02-25 00:49:10.535859: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: nyc-taxi model_base_path: /models/nyc-taxi\n",
    "2019-02-25 00:49:10.536171: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.\n",
    "2019-02-25 00:49:10.536244: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: nyc-taxi\n",
    "2019-02-25 00:49:10.653355: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: nyc-taxi version: 1551042367}\n",
    "2019-02-25 00:49:10.653463: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: nyc-taxi version: 1551042367}\n",
    "2019-02-25 00:49:10.653501: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: nyc-taxi version: 1551042367}\n",
    "2019-02-25 00:49:10.654161: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /models/nyc-taxi/1551042367\n",
    "2019-02-25 00:49:10.654272: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /models/nyc-taxi/1551042367\n",
    "2019-02-25 00:49:10.660314: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
    "2019-02-25 00:49:10.690752: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.\n",
    "2019-02-25 00:49:10.712442: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:138] Running MainOp with key saved_model_main_op on SavedModel bundle.\n",
    "2019-02-25 00:49:10.722428: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags { serve }; Status: success. Took 68199 microseconds.\n",
    "2019-02-25 00:49:10.723922: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:83] No warmup data file found at /models/nyc-taxi/1551042367/assets.extra/tf_serving_warmup_requests\n",
    "2019-02-25 00:49:10.738422: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: nyc-taxi version: 1551042367}\n",
    "2019-02-25 00:49:10.741308: I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ...\n",
    "[warn] getaddrinfo: address family for nodename not supported\n",
    "2019-02-25 00:49:10.743122: I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...\n",
    "[evhttp_server.cc : 237] RAW: Entering the event loop ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third party packages already installed!\n",
    "\n",
    "Third party dependencies can be found in `requirements.txt` and already have been installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using Tensorflow Serving\n",
    "\n",
    "In order to make predictions using our trained model, you will need to create a client that is able to send requests over [gRPC protocol](https://grpc.io/). \n",
    "\n",
    "If you look through the source code for `start_predict.sh`, you'll observe the following CLI command which sends a set of instances for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow_transform import coders as tft_coders\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "MAX_CATEGORICAL_FEATURE_VALUES = [2]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = []\n",
    "\n",
    "DENSE_FLOAT_FEATURE_KEYS = ['trip_distance', 'passenger_count', 'tip_amount']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "BUCKET_FEATURE_KEYS = ['pickup_hour', \n",
    "                       'pickup_month', \n",
    "                       'pickup_day_of_week', \n",
    "                       'dropoff_month',\n",
    "                       'dropoff_hour',\n",
    "                       'dropoff_day_of_week']\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "OOV_SIZE = 10\n",
    "\n",
    "VOCAB_FEATURE_KEYS = []\n",
    "\n",
    "LABEL_KEY = 'fare_amount'\n",
    "\n",
    "CSV_COLUMN_NAMES = [\n",
    "    'vendor_id',\n",
    "    'pickup_month',\n",
    "    'pickup_hour',\n",
    "    'pickup_day_of_week',\n",
    "    'dropoff_month',\n",
    "    'dropoff_hour',\n",
    "    'dropoff_day_of_week',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'fare_amount',\n",
    "    'tip_amount',\n",
    "    'payment_type',\n",
    "    'trip_type',]\n",
    "\n",
    "    # 'store_and_fwd_flag',\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "\n",
    "def transformed_names(keys):\n",
    "    return [transformed_name(key) for key in keys]\n",
    "\n",
    "\n",
    "# Tf.Transform considers these features as \"raw\"\n",
    "def get_raw_feature_spec(schema):\n",
    "    return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "\n",
    "def make_proto_coder(schema):\n",
    "    raw_feature_spec = get_raw_feature_spec(schema)\n",
    "    raw_schema = dataset_schema.from_feature_spec(raw_feature_spec)\n",
    "    return tft_coders.ExampleProtoCoder(raw_schema)\n",
    "\n",
    "\n",
    "def make_csv_coder(schema):\n",
    "    \"\"\"Return a coder for tf.transform to read csv files.\"\"\"\n",
    "    raw_feature_spec = get_raw_feature_spec(schema)\n",
    "    parsing_schema = dataset_schema.from_feature_spec(raw_feature_spec)\n",
    "    return tft_coders.CsvCoder(CSV_COLUMN_NAMES, parsing_schema)\n",
    "\n",
    "\n",
    "def clean_raw_data_dict(input_dict, raw_feature_spec):\n",
    "    \"\"\"Clean raw data dict.\"\"\"\n",
    "    output_dict = {}\n",
    "\n",
    "    for key in raw_feature_spec:\n",
    "        if key not in input_dict or not input_dict[key]:\n",
    "            output_dict[key] = []\n",
    "        else:\n",
    "            output_dict[key] = [input_dict[key]]\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def read_schema(path):\n",
    "    \"\"\"Reads a schema from the provided location.\n",
    "\n",
    "    Args:\n",
    "    path: The location of the file holding a serialized Schema proto.\n",
    "\n",
    "    Returns:\n",
    "    An instance of Schema or None if the input argument is None\n",
    "    \"\"\"\n",
    "    result = schema_pb2.Schema()\n",
    "    contents = file_io.read_file_to_string(path)\n",
    "    text_format.Parse(contents, result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import base64\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from grpc.beta import implementations\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "\n",
    "from tensorflow.python.lib.io import file_io  # pylint: disable=g-direct-tensorflow-import\n",
    "\n",
    "INFERENCE_TIMEOUT_SECONDS = 5.0\n",
    "\n",
    "\n",
    "def do_local_inference(host, port, serialized_examples):\n",
    "    \"\"\"Performs inference on a model hosted by the host:port server.\"\"\"\n",
    "\n",
    "    # create a connection\n",
    "    channel = implementations.insecure_channel(host, int(port))\n",
    "    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "    # initialize a request\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = 'nyc-taxi'\n",
    "    request.model_spec.signature_name = 'predict'\n",
    "\n",
    "    tfproto = tf.contrib.util.make_tensor_proto([serialized_examples],\n",
    "                                              shape=[len(serialized_examples)],\n",
    "                                              dtype=tf.string)\n",
    "    # The name of the input tensor is 'examples' based on\n",
    "    # https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/estimator/export/export.py#L306\n",
    "    request.inputs['examples'].CopyFrom(tfproto)\n",
    "    \n",
    "    # call predict\n",
    "    print(stub.Predict(request, INFERENCE_TIMEOUT_SECONDS))\n",
    "\n",
    "\n",
    "def do_inference(model_handle, examples_file, num_examples, schema):\n",
    "    \"\"\"Sends requests to the model and prints the results.\n",
    "\n",
    "    Args:\n",
    "    model_handle: handle to the model. This can be either\n",
    "     \"mlengine:model:version\" or \"host:port\"\n",
    "    examples_file: path to csv file containing examples, with the first line\n",
    "      assumed to have the column headers\n",
    "    num_examples: number of requests to send to the server\n",
    "    schema: a Schema describing the input data\n",
    "\n",
    "    Returns:\n",
    "    Response from model server\n",
    "    \"\"\"\n",
    "    filtered_features = [\n",
    "      feature for feature in schema.feature if feature.name != LABEL_KEY\n",
    "    ]\n",
    "    del schema.feature[:]\n",
    "    schema.feature.extend(filtered_features)\n",
    "\n",
    "    csv_coder = make_csv_coder(schema)\n",
    "    proto_coder = make_proto_coder(schema)\n",
    "\n",
    "    input_file = open(examples_file, 'r')\n",
    "    # skip header line\n",
    "    input_file.readline()  \n",
    "\n",
    "    serialized_examples = []\n",
    "    for _ in range(num_examples):\n",
    "        one_line = input_file.readline()\n",
    "        if not one_line:\n",
    "            print('End of example file reached')\n",
    "            break\n",
    "        one_example = csv_coder.decode(one_line)\n",
    "\n",
    "        serialized_example = proto_coder.encode(one_example)\n",
    "        serialized_examples.append(serialized_example)\n",
    "\n",
    "    parsed_model_handle = model_handle.split(':')\n",
    "    do_local_inference(\n",
    "      host=parsed_model_handle[0],\n",
    "      port=parsed_model_handle[1],\n",
    "      serialized_examples=serialized_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this next cell, make sure bash `./start_model_server.sh` is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs {\n",
      "  key: \"predictions\"\n",
      "  value {\n",
      "    dtype: DT_FLOAT\n",
      "    tensor_shape {\n",
      "      dim {\n",
      "        size: 15\n",
      "      }\n",
      "      dim {\n",
      "        size: 1\n",
      "      }\n",
      "    }\n",
      "    float_val: 32.9800186157\n",
      "    float_val: 17.2102165222\n",
      "    float_val: 31.7519054413\n",
      "    float_val: 33.0067481995\n",
      "    float_val: 36.803691864\n",
      "    float_val: 33.8259849548\n",
      "    float_val: 34.7675018311\n",
      "    float_val: 22.1285438538\n",
      "    float_val: 43.9800224304\n",
      "    float_val: 22.4845104218\n",
      "    float_val: 37.6693458557\n",
      "    float_val: 34.8329048157\n",
      "    float_val: 31.9676322937\n",
      "    float_val: 70.7163391113\n",
      "    float_val: 37.2004470825\n",
      "  }\n",
      "}\n",
      "model_spec {\n",
      "  name: \"nyc-taxi\"\n",
      "  version {\n",
      "    value: 1551254584\n",
      "  }\n",
      "  signature_name: \"predict\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arm/anaconda3/envs/py27_oreilly_ml_tfx_course/lib/python2.7/site-packages/tensorflow_serving/apis/prediction_service_pb2.py:131: DeprecationWarning: beta_create_PredictionService_stub() method is deprecated. This method will be removed in near future versions of TF Serving. Please switch to GA gRPC API in prediction_service_pb2_grpc.\n",
      "  'prediction_service_pb2_grpc.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "SERVER=\"127.0.0.1:9000\"\n",
    "SCHEMA_FILE=\"./schema.pbtxt\"\n",
    "NUM_INSTANCES=15\n",
    "INSTANCE_FILE=\"../data/train/train.csv\"\n",
    "\n",
    "do_inference(SERVER,\n",
    "              INSTANCE_FILE, \n",
    "              NUM_INSTANCES,\n",
    "              read_schema(SCHEMA_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command line interface to inspect a SavedModel\n",
    "\n",
    "A **SavedModel** contains one or more MetaGraphDefs, identified by their tag-sets. To serve a model, you might wonder what kind of SignatureDefs are in each model, and what are their inputs and outputs. The show command let you examine the contents of the SavedModel in hierarchical order. You can find more details [here](https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel).\n",
    "\n",
    "To inspect a SavedModel, run the following command through your terminal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n",
      "\r\n",
      "signature_def['predict']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['examples'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1)\r\n",
      "        name: input_example_tensor:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['predictions'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: dnn/logits/BiasAdd:0\r\n",
      "  Method name is: tensorflow/serving/predict\r\n",
      "\r\n",
      "signature_def['regression']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['inputs'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1)\r\n",
      "        name: input_example_tensor:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['outputs'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: dnn/logits/BiasAdd:0\r\n",
      "  Method name is: tensorflow/serving/regress\r\n",
      "\r\n",
      "signature_def['serving_default']:\r\n",
      "  The given SavedModel SignatureDef contains the following input(s):\r\n",
      "    inputs['inputs'] tensor_info:\r\n",
      "        dtype: DT_STRING\r\n",
      "        shape: (-1)\r\n",
      "        name: input_example_tensor:0\r\n",
      "  The given SavedModel SignatureDef contains the following output(s):\r\n",
      "    outputs['outputs'] tensor_info:\r\n",
      "        dtype: DT_FLOAT\r\n",
      "        shape: (-1, 1)\r\n",
      "        name: dnn/logits/BiasAdd:0\r\n",
      "  Method name is: tensorflow/serving/regress\r\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir=./tf/run_0/serving_model_dir/export/nyc-taxi/1551167001/ --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed TensorFlow through a pre-built TensorFlow binary, then the SavedModel CLI is already installed on your system. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
